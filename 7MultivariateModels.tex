\documentclass[12pt, compress]{beamer}

\usetheme{Warsaw}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage[spanish, english]{babel}
\usepackage{color}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage[round, comma]{natbib}
\usepackage{tabularx}
\usepackage{tabulary, comment, algpseudocode, algorithm, bm}
\usepackage{bbm}

\usepackage{tikz}
\usepackage{threeparttable}
\usepackage{caption}
\usetikzlibrary{positioning, arrows.meta}

\usepackage{etoolbox}

% Patch to allow non-floating algorithmic in beamer
\makeatletter
\patchcmd{\ALG@beginalgorithmic}{\begin{trivlist}}{\begin{minipage}{\linewidth}\tiny}{}{}
		\patchcmd{\ALG@endalgorithmic}{\end{trivlist}}{\end{minipage}}{}{}
\makeatother

% Reduce size of line numbers
\algrenewcommand\alglinenumber[1]{\scriptsize #1}

% Avoid floating in Beamer
\makeatletter
\setbeamertemplate{caption}[numbered]
\newcommand{\nonfloatalgorithm}[1]{%
	\par\vspace{\baselineskip}
	\noindent\begin{minipage}{\linewidth}
		\begin{algorithmic}[1] #1 \end{algorithmic}
	\end{minipage}\par\vspace{\baselineskip}
}
\makeatother

\let\Tiny \tiny
%\setbeamertemplate{background canvas}{\includegraphics[width = \paperwidth, height = \paperheight]{EAFIT.pdf}} % Include an image as part of the background
\setbeamertemplate{navigation symbols}{} % Getting rid of navigation symbols
\useoutertheme{tree}
\setbeamertemplate{footline}[frame number]

%%%%%%%%%%%%%%%%%%%%%%%% PRESENTACION %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Bayesian Econometrics}
\subtitle{Multivariate Models}
\date{2025}
\author[Andr\'e Ram\'irez H.]{\textbf{Andr\'es Ram\'irez-Hassan}}
\institute[EAFIT]{\small{Universidad EAFIT\\School of Finance, Economics and Government}}
%%%%%%%%%%%%%%%%%%%%%%% DOCUMENTO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\decimalpoint
%\justifying
\begin{document}
	
	%\tikzstyle{every picture}+=[remember picture]
	%\everymath{\displaystyle}
	%\tikzstyle{na} = [baseline=-.5ex]
	\maketitle
	
	% \begin{frame}
		% \includegraphics[width= 0.15\linewidth]{escudo.eps}
		% \maketitle
		% \end{frame}
	
	
	
	\section*{Outline}
	\begin{frame}
		\textbf{\Large{Outline}}
		\tableofcontents
	\end{frame}
	
\section{Introduction}
	
\begin{frame}
	\frametitle{Introduction}
We describe how to perform Bayesian inference in multivariate response models, including multivariate regression, seemingly unrelated regression, and instrumental variable. In particular, we present the posterior distributions of the parameters and illustrate their use through selected applications and simulations.
\end{frame}

\section{Models}
\begin{frame}
	\frametitle{Multivariare regression}
There are $M$ jointly dependent variables that share the same set of regressors, and their stochastic errors are contemporaneously correlated. Specifically, let $\bm{Y} = \left[ \bm{y}_1 \ \bm{y}_2 \ \ldots \ \bm{y}_M \right]$ denote an $N \times M$ matrix generated by
\[
\bm{Y} = \bm{X}\bm{B} + \bm{U},
\]
where $\bm{X}$ is an $N \times K$ matrix of regressors, $\bm{B} = \left[ \bm{\beta}_1 \ \bm{\beta}_2 \ \ldots \ \bm{\beta}_M \right]$ is a $K \times M$ matrix of coefficients, and $\bm{U} = \left[ \bm{\mu}_1 \ \bm{\mu}_2 \ \ldots \ \bm{\mu}_M \right]$ is an $N \times M$ matrix of stochastic errors such that each row $\bm{\mu}_i \stackrel{i.i.d.}{\sim} N(\bm{0}, \bm{\Sigma})$ for $i = 1, 2, \dots, N$.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression}
The prior distributions are specified as $\bm{B} \mid \bm{\Sigma} \sim N(\bm{B}_0, \bm{V}_0, \bm{\Sigma})$ and $\bm{\Sigma} \sim IW(\bm{\Psi}_0, \alpha_0)$. The conditional posterior distributions are then given by
\begin{equation*}
	\bm{B} \mid \bm{\Sigma}, \bm{Y}, \bm{X} \sim N(\bm{B}_n, \bm{V}_n, \bm{\Sigma}),
\end{equation*}
\begin{equation*}
	\bm{\Sigma} \mid \bm{Y}, \bm{X} \sim IW(\bm{\Psi}_n, \alpha_n),
\end{equation*}
where $\bm{V}_n = (\bm{X}^{\top}\bm{X} + \bm{V}_0^{-1})^{-1}$, $\bm{B}_n = \bm{V}_n(\bm{V}_0^{-1}\bm{B}_0 + \bm{X}^{\top}\bm{X}\hat{\bm{B}})$, $\hat{\bm{B}} = (\bm{X}^{\top}\bm{X})^{-1}\bm{X}^{\top}\bm{Y}$, ${\bm{S}} = (\bm{Y} - \bm{X}\hat{\bm{B}})^{\top}(\bm{Y} - \bm{X}\hat{\bm{B}})$, $\bm{\Psi}_n = \bm{\Psi}_0 + \bm{S} + \bm{B}_0^{\top}\bm{V}_0^{-1}\bm{B}_0 + \hat{\bm{B}}^{\top}\bm{X}^{\top}\bm{X}\hat{\bm{B}} - \bm{B}_n^{\top}\bm{V}_n^{-1}\bm{B}_n$, and $\alpha_n = \alpha_0 + N$.

Since the conditional posterior distributions are standard, inference in this model can be carried out using a Gibbs sampling algorithm.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
We begin with the following \textit{simultaneous structural} economic model:
\begin{align}\label{eq:str1}
	\log(\text{pcGDP95}_i) &= \beta_1 + \beta_2 \text{PAER}_i + \beta_3 \text{Africa}_i + \beta_4 \text{Asia}_i\nonumber \\
    &+ \beta_5 \text{Other}_i + u_{1i},\\
	\text{PAER}_i &= \alpha_1 + \alpha_2 \log(\text{pcGDP95}_i) + \alpha_3 \log(\text{Mort}_i) + u_{2i}. \label{eq:str2}
\end{align}
Here, \textit{pcGDP95}, \textit{PAER}, and \textit{Mort} denote, respectively, the per capita gross domestic product in 1995, the average index of protection against expropriation between 1985 and 1995, and the settler mortality rate during the period of colonization. \textit{Africa}, \textit{Asia}, and \textit{Other} are continent indicators, with \textit{America} serving as the baseline category.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
Substituting Equation~\ref{eq:str2} into Equation~\ref{eq:str1} and solving for $\log(\text{pcGDP95}_i)$ yields:
\begin{align}\label{eq:red1}
	\log(\text{pcGDP95}_i) &= \pi_1 + \pi_2 \log(\text{Mort}_i) + \pi_3 \text{Africa}_i + \pi_4 \text{Asia}_i\nonumber\\ 
    &+ \pi_5 \text{Other}_i + e_{1i}.
\end{align}
Then, substituting Equation~\ref{eq:red1} into Equation~\ref{eq:str2} and solving for \textit{PAER} gives:
\begin{align}\label{eq:red2}
	\text{PAER}_i = \gamma_1 + \gamma_2 \log(\text{Mort}_i) + \gamma_3 \text{Africa}_i + \gamma_4 \text{Asia}_i + \gamma_5 \text{Other}_i + e_{2i},
\end{align}
where $\pi_2 = \frac{\beta_2 \alpha_3}{1 - \beta_2 \alpha_2}$ and $\gamma_2 = \frac{\alpha_3}{1 - \beta_2 \alpha_2}$, assuming $\beta_2 \alpha_2 \neq 1$, which ensures model identification.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
\textbf{The order condition}

Given a system of equations with $M$ endogenous variables and $K$ exogenous variables (including the intercept), there are two ways to assess the order condition:
\begin{itemize}
	\item The parameters of an equation in the system are identified if there are at least $M - 1$ variables excluded from that equation (\textit{exclusion restrictions}). The equation is \textit{exactly identified} if the number of excluded variables equals $M - 1$, and \textit{over-identified} if the number of excluded variables is greater than $M - 1$.
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
\textbf{The order condition}

\begin{itemize}
	\item The parameters of equation $m$ in the system are identified if $K - K_m \geq M_m - 1$, where $K_m$ and $M_m$ denote the number of exogenous and endogenous variables in equation $m$, respectively. The $m$th equation is \textit{exactly identified} if $K - K_m = M_m - 1$, and \textit{over-identified} if $K - K_m > M_m - 1$.
\end{itemize}

Otherwise, the structural parameters are said to be \textit{under-identified}.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
\textbf{The rank condition}

The rank condition (necessary and sufficient) states that, given a \textit{structural} model with $M$ equations (and $M$ endogenous variables), an equation is identified if and only if there exists at least one nonzero determinant of an $(M - 1) \times (M - 1)$ matrix constructed from the variables that are excluded from the equation under analysis but included in at least one other equation of the system. Otherwise, the structural parameters are said to be \textit{under-identified}.
\end{frame}

\begin{frame}
	\frametitle{Multivariare regression: The effect of institutions on per capita gross domestic product}
\textbf{The rank condition}

{\footnotesize{
\begin{table}[!h]
	%\noautomaticrules
	\caption{Identification matrix.}\label{tab:71}%
	\begin{tabular}{ccccccc}
		$\log(\text{pcGDP95})$ & PAER & Constant & $\log(\text{Mort})$ & Africa & Asia & Other \\
		\hline
		1 & -$\beta_2$ & -$\beta_1$ & 0 & -$\beta_3$ & -$\beta_4$ & -$\beta_5$\\
		-$\alpha_2$ & 1 & $-\alpha_1$ & -$\alpha_3$ & 0 & 0 & 0 \\
        \hline
	\end{tabular}
\end{table}
}}
\end{frame}

\begin{frame}
	\frametitle{Seemingly unrelated regression (SUR)}
In seemingly unrelated regression (SUR) models, there are $M$ dependent variables, each with potentially different sets of regressors, and their stochastic errors are contemporaneously correlated. The model is given by
\[
\bm{y}_m = \bm{X}_m \bm{\beta}_m + \bm{\mu}_m,
\]
where $\bm{y}_m$ is an $N$-dimensional vector of observations, $\bm{X}_m$ is an $N \times K_m$ matrix of regressors, $\bm{\beta}_m$ is a $K_m$-dimensional vector of coefficients, and $\bm{\mu}_m$ is an $N$-dimensional vector of stochastic errors, for $m = 1, 2, \dots, M$. Let $\bm{\mu}_i = \left[\mu_{i1} \ \mu_{i2} \ \dots \ \mu_{iM}\right]^{\top}$, where $\bm{\mu}_i \sim {N}(\bm{0}, \bm{\Sigma})$.
\end{frame}

\begin{frame}
	\frametitle{Seemingly unrelated regression (SUR)}
Stacking the $M$ equations, the system can be written as
\[
\bm{y} = \bm{X} \bm{\beta} + \bm{\mu},
\]
where $\bm{y} = \left[\bm{y}_1^{\top} \ \bm{y}_2^{\top} \ \dots \ \bm{y}_M^{\top}\right]^{\top}$ is an $MN$-dimensional vector,
$\bm{\beta} = \left[\bm{\beta}_1^{\top} \ \bm{\beta}_2^{\top} \ \dots \ \bm{\beta}_M^{\top}\right]^{\top}$ is a $K$-dimensional vector with $K = \sum_{m=1}^M K_m$, and $\bm{X}$ is an $MN \times K$ block-diagonal matrix composed of the individual $\bm{X}_m$ matrices:
\[
\bm{X} =
\begin{bmatrix}
	\bm{X}_1 & \bm{0} & \dots & \bm{0} \\
	\bm{0} & \bm{X}_2 & \dots & \bm{0} \\
	\vdots & \vdots & \ddots & \vdots \\
	\bm{0} & \bm{0} & \dots & \bm{X}_M
\end{bmatrix}.
\]
$\bm{\mu} = \left[\bm{\mu}_1^{\top} \ \bm{\mu}_2^{\top} \ \dots \ \bm{\mu}_M^{\top}\right]^{\top}$, an $MN$-dimensional vector with $\bm{\mu} \sim {N}(\bm{0}, \bm{\Sigma} \otimes \bm{I}_N)$.
\end{frame}

\begin{frame}
	\frametitle{Seemingly unrelated regression (SUR)}
The likelihood function for the parameters is
\begin{align*}
p(\bm{\beta}, \bm{\Sigma} \mid \bm{y}, \bm{X}) & \propto |\bm{\Sigma}|^{-N/2}\\
& \times
\exp\left\{ -\frac{1}{2} (\bm{y} - \bm{X} \bm{\beta})^{\top}
(\bm{\Sigma}^{-1} \otimes \bm{I}_N) (\bm{y} - \bm{X} \bm{\beta}) \right\}.
\end{align*}

Assuming independent priors, $\pi(\bm{\beta}) \sim {N}(\bm{\beta}_0, \bm{B}_0)$ and
$\pi(\bm{\Sigma}^{-1}) \sim {W}(\alpha_0, \bm{\Psi}_0)$.
\end{frame}

\begin{frame}
	\frametitle{Seemingly unrelated regression (SUR)}
the posterior distributions are
\begin{equation*}
	\bm{\beta} \mid \bm{\Sigma}, \bm{y}, \bm{X} \sim {N}(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\Sigma}^{-1} \mid \bm{\beta}, \bm{y}, \bm{X} \sim {W}(\alpha_n, \bm{\Psi}_n),
\end{equation*}
where
\(
\bm{B}_n = \left(\bm{X}^{\top} (\bm{\Sigma}^{-1} \otimes \bm{I}_N) \bm{X} + \bm{B}_0^{-1}\right)^{-1},\)
\(
\bm{\beta}_n = \bm{B}_n \left(\bm{B}_0^{-1}\bm{\beta}_0 + \bm{X}^{\top}(\bm{\Sigma}^{-1} \otimes \bm{I}_N)\bm{y}\right),
\)
\(
\alpha_n = \alpha_0 + N,\) and \(\bm{\Psi}_n = \left(\bm{\Psi}_0^{-1} + \bm{U}^{\top}\bm{U}\right)^{-1},
\)
where $\bm{U}$ is an $N \times M$ matrix whose columns are given by
$\bm{y}_m - \bm{X}_m \bm{\beta}_m$.
\end{frame}

\begin{frame}
	\frametitle{Seemingly unrelated regression (SUR): Utility demand}
{\scriptsize{
\begin{align*}
	\log(\text{electricity}_i) & = \beta_1 + \beta_2\log(\text{electricity price}_i)+\beta_3\log(\text{water price}_i)\\
	&+\beta_4\log(\text{gas price}_i)+\beta_5\text{IndSocio1}_i+\beta_6\text{IndSocio2}_i+\beta_7\text{Altitude}_i\\
	&+\beta_8\text{Nrooms}_i+\beta_9\text{HouseholdMem}_i+\beta_{10}\log(\text{Income}_i)+\mu_{i1}\\
	\log(\text{water}_i) & = \alpha_1 + \alpha_2\log(\text{electricity price}_i)+\alpha_3\log(\text{water price}_i)\\
	&+\alpha_4\log(\text{gas price}_i)+\alpha_5\text{IndSocio1}_i+\alpha_6\text{IndSocio2}_i\\
	&+\alpha_7\text{Nrooms}_i+\alpha_8\text{HouseholdMem}_i+\mu_{i2}\\
	\log(\text{gas}_i) & = \gamma_1 + \gamma_2\log(\text{electricity price}_i)+\gamma_3\log(\text{water price}_i)\\
	&+\gamma_4\log(\text{gas price}_i)+\gamma_5\text{IndSocio1}_i+\gamma_6\text{IndSocio2}_i+\gamma_7\text{Altitude}_i\\
	&+\gamma_8\text{Nrooms}_i+\gamma_9\text{HouseholdMem}_i+\mu_{i3}.
\end{align*} 
}}
\end{frame}

\begin{frame}
	\frametitle{Instrumental variable}
This inferential approach is used when there are \textit{endogeneity} issues, that is, when the stochastic error term is not independent of the regressors. This dependence generates \textit{bias} in posterior mean estimates when using an inferential approach that does not account for it.
Let the dependent variable be modeled as a linear function of one endogenous regressor and a set of exogenous regressors:
\[
y_{i} = \bm{x}_{ei}^{\top}\bm{\beta}_1 + \beta_s x_{si} + \mu_{i},
\]
where
\[
x_{si} = \bm{x}_{ei}^{\top}\bm{\gamma}_1 + \bm{z}_i^{\top}\bm{\gamma}_2 + v_{i}.
\]
Here, \(x_{si}\) is the variable that generates endogeneity (\(\mathbb{E}[\mu \mid x_s] \neq 0\)), \(\bm{x}_e\) is a vector of \(K_1\) exogenous regressors (\(\mathbb{E}[\mu \mid \bm{x}_e] = \bm{0}\)), and \(\bm{z}\) is a vector of \(K_2\) instruments.
\end{frame}

\begin{frame}
	\frametitle{Instrumental variable}
The instruments influence \(x_s\) (\(\mathbb{E}[x_s \bm{z}] \neq \bm{0}\)) but do not directly affect \(y\) (\(\mathbb{E}[y \bm{z} \mid x_s] = \bm{0}\)). The equation for \(y\) is known as the \textit{structural equation}, which is the main object of inference.

Assuming $(\mu_i, v_i)^{\top} \stackrel{i.i.d.}{\sim} N(\bm{0}, \bm{\Sigma})$, with $\bm{\Sigma} = [\sigma_{lm}]$, $l,m = 1,2$, the likelihood function is
{\scriptsize{
\begin{align*}
	p(\bm{\beta}, \bm{\gamma}, \bm{\Sigma} \mid \bm{y}, \bm{X}, \bm{Z}) 
	&= \frac{1}{(2\pi)^{N/2} |\bm{\Sigma}|^{N/2}}\\
	&\times
	\exp\left\{
	-\frac{1}{2} \sum_{i=1}^N 
	(y_i - \bm{x}_i^{\top}\bm{\beta},\, x_{si} - \bm{w}_i^{\top}\bm{\gamma})\bm{\Sigma}^{-1}
	\begin{pmatrix}
		y_i - \bm{x}_i^{\top}\bm{\beta} \\
		x_{si} - \bm{w}_i^{\top}\bm{\gamma}
	\end{pmatrix}
	\right\},
\end{align*}
}}
where $\bm{\beta} = [\bm{\beta}_1^{\top} \ \beta_s]^{\top}$, $\bm{\gamma} = [\bm{\gamma}_1^{\top} \ \bm{\gamma}_2^{\top}]^{\top}$, $\bm{x}_i = [\bm{x}_{ei}^{\top} \ x_{si}]^{\top}$, and $\bm{w}_i = [\bm{x}_{ei}^{\top} \ \bm{z}_{i}^{\top}]^{\top}$.\\
Given independent priors:
\[
\bm{\gamma} \sim N(\bm{\gamma}_0, \bm{G}_0), 
\quad 
\bm{\beta} \sim N(\bm{\beta}_0, \bm{B}_0), 
\quad 
\bm{\Sigma}^{-1} \sim W(\alpha_0, \bm{\Psi}_0).
\]
\end{frame}

\begin{frame}
	\frametitle{Instrumental variable}
The conditional posterior distributions are:
\begin{equation*}
	\bm{\beta} \mid \bm{\gamma}, \bm{\Sigma}, \bm{y}, \bm{X}, \bm{Z} \sim N(\bm{\beta}_n, \bm{B}_n),
\end{equation*}
\begin{equation*}
	\bm{\gamma} \mid \bm{\beta}, \bm{\Sigma}, \bm{y}, \bm{X}, \bm{Z} \sim N(\bm{\gamma}_n, \bm{G}_n),
\end{equation*}
\begin{equation*}
	\bm{\Sigma}^{-1} \mid \bm{\beta}, \bm{\gamma}, \bm{y}, \bm{X}, \bm{Z} \sim W(\alpha_n, \bm{\Psi}_n),
\end{equation*}
where
{\scriptsize{
\(
\bm{\beta}_n = \bm{B}_n \left( \bm{B}_0^{-1} \bm{\beta}_0 
+ \omega_1^{-1} \sum_{i=1}^{N} 
\bm{x}_i \left( y_i - \frac{\sigma_{12}(x_{si} - \bm{w}_i^{\top}\bm{\gamma})}{\sigma_{22}} \right) \right),
\)
\(
\bm{B}_n = \left( \omega_1^{-1} \sum_{i=1}^{N} \bm{x}_i \bm{x}_i^{\top} + \bm{B}_0^{-1} \right)^{-1}, \omega_1 = \sigma_{11} - \frac{\sigma_{12}^2}{\sigma_{22}},
\)
\(
\bm{G}_n = \left( \omega_2^{-1} \sum_{i=1}^{N} \bm{w}_i \bm{w}_i^{\top} + \bm{G}_0^{-1} \right)^{-1},\) 
\(\bm{\gamma}_n = \bm{G}_n \left( \bm{G}_0^{-1} \bm{\gamma}_0 
+ \omega_2^{-1} \sum_{i=1}^{N} 
\bm{w}_i \left( x_{si} - \frac{\sigma_{12}(y_i - \bm{x}_i^{\top}\bm{\beta})}{\sigma_{11}} \right) \right),
\)
\(
\omega_2 = \sigma_{22} - \frac{\sigma_{12}^2}{\sigma_{11}}, 
\quad 
\bm{\Psi}_n = \left[ \bm{\Psi}_0^{-1} 
+ \sum_{i=1}^N 
\begin{pmatrix} 
	y_i - \bm{x}_i^{\top}\bm{\beta} \\ 
	x_{si} - \bm{w}_i^{\top}\bm{\gamma} 
\end{pmatrix} 
(y_i - \bm{x}_i^{\top}\bm{\beta},\, x_{si} - \bm{w}_i^{\top}\bm{\gamma}) 
\right]^{-1},
\)
\(
\alpha_n = \alpha_0 + N,
\) and $\sigma_{lm}$ are the elements of $\bm{\Sigma}$.
}}
\end{frame}

\begin{frame}
	\frametitle{Instrumental variable: Simulation exercise}
Let's simulate the simple process $y_i=\beta_1+\beta_2x_{si}+\mu_i$ and $x_{si}=\gamma_1+\gamma_2z_i+v_i$ where $[\mu_i \ v_i]^{\top}\sim N(\bm{0},\bm{\Sigma})$, $\bm{\Sigma}=[\sigma_{lj}]$ such that $\sigma_{12}\neq 0$, $i=1,2,\dots,100$.

Observe that $\mu \mid v \sim N\left(\frac{\sigma_{12}}{\sigma_{22}}v,\ \sigma_{11} - \frac{\sigma_{21}^2}{\sigma_{22}}\right)$. This implies that $\mathbb{E}[\mu \mid x_s] = \mathbb{E}[\mu \mid v] = \frac{\sigma_{12}}{\sigma_{22}}v \neq 0$ when $\sigma_{12} \neq 0$, while $\mathbb{E}[\mu \mid z] = 0$.

Let all location parameters be equal to 1, and set $\sigma_{11} = \sigma_{22} = 1$, $\sigma_{12} = 0.8$, and $z \sim N(0,1)$.
\end{frame}

\begin{frame}
	\frametitle{Instrumental variable: Simulation exercise}
From the large-sample properties of the posterior mean, we know that it converges to the maximum likelihood estimator. In this setting, the estimator is 
\[
\hat{\beta}_2 = \frac{\widehat{\mathrm{Cov}}(x_s, y)}{\widehat{\mathrm{Var}}(x_s)},
\]
which converges in probability to 
\[
\beta_2 + \frac{\sigma_{12}}{\sigma_{22}\mathrm{Var}(x_s)} 
= \beta_2 + \frac{\sigma_{12}}{\sigma_{22}(\gamma_2^2 \mathrm{Var}(z) + \sigma_{22})} 
= 1.4.
\]
\end{frame}

%\begin{frame}[allowframebreaks]
%	\frametitle{References}
%		{\footnotesize
%		\bibliographystyle{apalike}
%		\bibliography{Biblio}}
%\end{frame}
	
\end{document}